---
title: B+tree Implementation
description: Detailed technical documentation of NorthstarDB's B+tree implementation, including node structure, split/merge algorithms, cursor operations, and performance characteristics.
---

# B+tree Implementation

NorthstarDB stores all ordered key-value data in a **copy-on-write B+tree**. This document details the implementation, algorithms, and performance characteristics.

## Overview

The B+tree provides:
- **Ordered key-value storage** with lexicographic key comparison
- **Efficient point lookups**: O(log<sub>512</sub>(N)) for 16KB pages
- **Fast range scans**: O(K + log<sub>512</sub>(N)) where K = results
- **Copy-on-write semantics**: Updates create new pages, enabling MVCC

## Node Structure

### Common Node Header

All B+tree nodes (leaf and internal) share a common header:

```zig
pub const BtreeNodeHeader = struct {
    node_magic: u32 = 0x42545245,    // "BTRE"
    level: u16,                       // 0 = leaf, >0 = internal
    key_count: u16,                   // Number of keys
    right_sibling: u64,               // Right sibling page (0 = none)
    reserved: [32]u8,                 // Reserved for future use
};
```

### Page Layout

```
┌────────────────────────────────────────────────┐
│  PageHeader (48 bytes)                         │
│  - magic: 0x4E534442 ("NSDB")                  │
│  - page_type: btree_leaf or btree_internal     │
│  - page_id, txn_id, checksums                  │
├────────────────────────────────────────────────┤
│  BtreeNodeHeader (44 bytes)                    │
│  - node_magic: 0x42545245 ("BTRE")             │
│  - level: 0 for leaf, >0 for internal          │
│  - key_count: number of keys                   │
│  - right_sibling: page ID for scans            │
├────────────────────────────────────────────────┤
│  Node-Specific Payload (variable)              │
│  - Leaf: slot array + entries                  │
│  - Internal: children + separators             │
├────────────────────────────────────────────────┤
│  Free space                                    │
└────────────────────────────────────────────────┘
```

## Leaf Nodes

Leaf nodes store the actual key-value pairs using a **slotted page format**.

### Slotted Page Layout

```
┌────────────────────────────────────────────────┐
│  BtreeNodeHeader                               │
├────────────────────────────────────────────────┤
│  Slot Array (grows forward)                    │
│  [slot 0 offset] [slot 1 offset] ...           │
├────────────────────────────────────────────────┤
│  Free Space (shrinks as entries added)         │
├────────────────────────────────────────────────┤
│  Entry Data Area (grows backward)              │
│  ... [entry 2] [entry 1] [entry 0]            │
└────────────────────────────────────────────────┘
```

### Entry Format

Each entry contains a key-value pair:

```zig
pub const BtreeLeafEntry = struct {
    key_len: u16,      // Key length in bytes
    val_len: u32,      // Value length in bytes (0 = deleted)
    key: []u8,         // Key bytes
    value: []u8,       // Value bytes (empty if deleted)
};
```

**Encoding**:
```
[key_len:2][val_len:4][key_bytes...][value_bytes...]
```

### Slot Array Operations

The slot array provides **O(1) indexed access** to entries:

```zig
// Get entry at slot index
pub fn getEntry(payload: []u8, slot_idx: u16) !BtreeLeafEntry {
    const node_header = readNodeHeader(payload);
    if (slot_idx >= node_header.key_count)
        return error.SlotIndexOutOfBounds;

    const slot_array = readSlotArray(payload);
    const entry_offset = slot_array[slot_idx];
    return readEntry(payload[entry_offset..]);
}

// Add entry (maintains sorted order)
pub fn addEntry(payload: []u8, key: []u8, value: []u8) !u16 {
    // 1. Calculate entry size
    // 2. Find space before existing entries
    // 3. Write entry data
    // 4. Find insertion position (binary search)
    // 5. Shift slots to make room
    // 6. Insert new slot
    // 7. Update key_count
}

// Remove entry
pub fn removeEntry(payload: []u8, key: []u8) !bool {
    // 1. Find key via binary search
    // 2. Shift slots down to remove
    // 3. Update key_count
}
```

### Leaf Node Constants

```zig
// Maximum capacity (conservative for 16KB page)
pub const MAX_KEYS_PER_LEAF: usize = 200;

// Minimum keys before merge (50% fill factor)
pub const MIN_KEYS_PER_LEAF: usize = 100;
```

## Internal Nodes

Internal nodes store **separator keys and child pointers** for tree traversal.

### Internal Node Layout

```
┌────────────────────────────────────────────────┐
│  BtreeNodeHeader (level > 0)                   │
├────────────────────────────────────────────────┤
│  Child 0 PageId (8 bytes)                      │
│  Separator 0: [key_len][key...]               │
│  Child 1 PageId (8 bytes)                      │
│  Separator 1: [key_len][key...]               │
│  ...                                           │
│  Child N PageId (8 bytes)                      │
└────────────────────────────────────────────────┘
```

### Invariant

**Key range invariant**: All keys in `child[i]` are less than `separator[i]`, and all keys in `child[i+1]` are greater than or equal to `separator[i]`.

Example:
```
Internal node at level 1:
  child[0] → keys < "banana"
  separator[0] = "banana"
  child[1] → keys >= "banana" and < "cherry"
  separator[1] = "cherry"
  child[2] → keys >= "cherry"
```

### Search in Internal Node

```zig
pub fn findChild(internal: []const u8, key: []const u8) !u64 {
    const node_header = readNodeHeader(internal);
    var child_idx: usize = 0;

    // Binary search for separator
    while (child_idx < node_header.key_count) {
        const separator = readSeparator(internal, child_idx);
        if (std.mem.lessThan(u8, key, separator)) {
            break;
        }
        child_idx += 1;
    }

    return readChildPageId(internal, child_idx);
}
```

## Split Algorithm

When a node overflows (key_count > MAX_KEYS), it must be **split** into two nodes.

### Leaf Split

```zig
pub fn splitLeaf(overflow_leaf: []u8) !struct { left: []u8, right: []u8, separator: []const u8 } {
    const node_header = readNodeHeader(overflow_leaf);
    const split_point = node_header.key_count / 2;

    // Create left leaf (keys [0..split_point))
    var left_leaf = allocateNewPage();
    copyEntries(overflow_leaf, left_leaf, 0, split_point);

    // Create right leaf (keys [split_point..key_count))
    var right_leaf = allocateNewPage();
    copyEntries(overflow_leaf, right_leaf, split_point, node_header.key_count);

    // Separator is first key in right leaf
    const separator = readFirstKey(right_leaf);

    // Link siblings
    left_leaf.right_sibling = right_leaf.page_id;
    right_leaf.right_sibling = overflow_leaf.right_sibling;

    return .{
        .left = left_leaf,
        .right = right_leaf,
        .separator = separator,
    };
}
```

**Split example**:
```
Before: [A, B, C, D, E, F] (6 keys, overflow)

Split at 3:
  Left:  [A, B, C]
  Right: [D, E, F]
  Separator: D (first key of right)

Parent update:
  Insert separator "D" with pointer to right leaf
```

### Internal Node Split

```zig
pub fn splitInternal(overflow_internal: []u8) !struct {
    left: []u8,
    right: []u8,
    separator: []const u8
} {
    const split_point = findSplitPoint(overflow_internal);

    // Left internal: children [0..split_point], separators [0..split_point-1]
    // Right internal: children [split_point..N], separators [split_point..N-1]

    // Separator to promote: separator at split_point - 1
    const promote_separator = readSeparator(overflow_internal, split_point - 1);

    return .{
        .left = left_internal,
        .right = right_internal,
        .separator = promote_separator,
    };
}
```

### Path Copy

When splitting, **all nodes on the path from root to leaf are copied** (COW):

```
Insert "F" into tree:
  [Root: A, C] → splits at leaf level

Path copy:
1. Split leaf [D, E] + [F] → [D, E] | [F]
2. Copy parent [A, C] → [A, C, F] (promote "F")
3. If parent overflows, repeat upward

Result: New root created at each level of split
```

## Merge Algorithm

When a node underflows (key_count < MIN_KEYS), it may be **merged** with a sibling.

### Leaf Merge

```zig
pub fn mergeLeaves(left: []u8, right: []u8) ![]u8 {
    const left_header = readNodeHeader(left);
    const right_header = readNodeHeader(right);

    if (left_header.key_count + right_header.key_count > MAX_KEYS_PER_LEAF)
        return error.CannotMerge;

    // Copy all entries from right to left
    for (0..right_header.key_count) |i| {
        const entry = getEntry(right, i);
        _ = try addEntry(left, entry.key, entry.value);
    }

    // Update sibling pointer
    left.right_sibling = right.right_sibling;

    return left;
}
```

**Merge example**:
```
Before:
  Left:  [A, B] (underflow: 2 < 100)
  Right: [C, D]

After merge:
  Merged: [A, B, C, D]
  Parent separator "C" removed
```

### Redistribution Alternative

If merge would overflow, **redistribute** entries instead:

```zig
pub fn redistributeLeaves(left: []u8, right: []u8) !void {
    const left_header = readNodeHeader(left);
    const right_header = readNodeHeader(right);
    const total = left_header.key_count + right_header.key_count;

    if (total <= MAX_KEYS_PER_LEAF) {
        // Merge is better
        return try mergeLeaves(left, right);
    }

    // Redistribute evenly
    const target_left = total / 2;
    const move_count = target_left - left_header.key_count;

    // Move entries from right to left
    for (0..move_count) |_| {
        const entry = getFirstEntry(right);
        _ = try removeFirstEntry(right);
        _ = try addEntry(left, entry.key, entry.value);
    }

    // Update parent separator
    updateParentSeparator(left, right);
}
```

## Cursor and Scanning

NorthstarDB provides **bidirectional cursors** for efficient range scans.

### Cursor Structure

```zig
pub const Cursor = struct {
    db: *Db,
    snapshot: Snapshot,
    stack: std.ArrayList(StackEntry),

    const StackEntry = struct {
        page_id: u64,
        slot_idx: u16,
    };
};
```

### Cursor Operations

```zig
// Seek to key (or next greater)
pub fn seek(cursor: *Cursor, key: []const u8) !?KV {
    // 1. Traverse from root to leaf
    // 2. Find key in leaf (or insertion point)
    // 3. Save path in stack for next/prev
}

// Move to next entry
pub fn next(cursor: *Cursor) !?KV {
    const stack_top = &cursor.stack.items[cursor.stack.items.len - 1];

    // Try next slot in current leaf
    if (stack_top.slot_idx + 1 < getCurrentLeafKeyCount()) {
        stack_top.slot_idx += 1;
        return getCurrentLeafEntry();
    }

    // Follow right sibling
    if (getCurrentLeaf().right_sibling != 0) {
        stack_top.page_id = getCurrentLeaf().right_sibling;
        stack_top.slot_idx = 0;
        return getCurrentLeafEntry();
    }

    // End of scan
    return null;
}

// Move to previous entry
pub fn prev(cursor: *Cursor) !?KV {
    // Similar to next, but:
    // 1. Try previous slot in current leaf
    // 2. If at slot 0, go to parent and find left sibling
}
```

### Range Scan

```zig
pub fn rangeScan(txn: *ReadTxn, start: []const u8, end: []const u8) ![]KV {
    var cursor = try txn.openCursor();
    defer cursor.close();

    // Seek to start key
    _ = try cursor.seek(start);

    var results = std.ArrayList(KV).init(txn.allocator);
    while (try cursor.next()) |kv| {
        if (std.mem.greaterThan(u8, kv.key, end)) break;
        try results.append(kv);
    }

    return results.toOwnedSlice();
}
```

### Prefix Scan

```zig
pub fn prefixScan(txn: *ReadTxn, prefix: []const u8) ![]KV {
    // Calculate next prefix (e.g., "user:" → "user;")
    var end_prefix = try txn.allocator.dupe(u8, prefix);
    defer txn.allocator.free(end_prefix);

    // Increment last character
    const last_idx = end_prefix.len - 1;
    end_prefix[last_idx] += 1;

    return rangeScan(txn, prefix, end_prefix);
}
```

## Performance Characteristics

### Theoretical Complexity

| Operation | Complexity | Notes |
|-----------|------------|-------|
| Point lookup | O(log<sub>b</sub>(N)) | b ≈ 512 for 16KB pages |
| Range scan (K results) | O(K + log<sub>b</sub>(N)) | Sibling links optimize |
| Insert | O(log<sub>b</sub>(N)) | Path copy: O(height) pages |
| Delete | O(log<sub>b</sub>(N)) | Merge may require O(height) |
| Space | O(N) | Leaf pages + internal levels |

### Practical Benchmarks (16KB pages)

| Dataset Size | Tree Height | Point Lookup | Range Scan (1000) |
|--------------|-------------|--------------|-------------------|
| 1K keys | 1 | ~1 µs | ~100 µs |
| 100K keys | 2 | ~2 µs | ~200 µs |
| 10M keys | 3 | ~3 µs | ~300 µs |
| 1B keys | 4 | ~4 µs | ~400 µs |

### Space Efficiency

- **Page overhead**: 92 bytes (48 header + 44 node header)
- **Slot overhead**: 2 bytes per entry
- **Entry overhead**: 6 bytes per entry (2 key_len + 4 val_len)
- **Utilization**: ~90% for random inserts, ~95% for sequential

### Write Amplification

- **Path copy**: O(height) pages per update
- **Split cost**: Additional page for split sibling
- **Random updates**: ~2x space (old + new versions)
- **Sequential updates**: ~1.1x space (better page reuse)

## Optimization Techniques

### Right Sibling Links

Leaf nodes are linked for efficient range scans:

```
Leaf A → Leaf B → Leaf C

Range scan [apple, cherry]:
1. Traverse to leaf A (contains apple)
2. Scan entries in A
3. Follow sibling to B, scan entries
4. Follow sibling to C, scan until cherry
5. Stop (cherry found)
```

**Benefit**: O(K) scan instead of O(K × height) re-traversal

### Prefix Key Compression (Future)

V0 stores full keys. V1 may implement **prefix compression**:

```
Current: ["user:001", "user:002", "user:003"]
Prefix compressed: ["user:001", "002", "003"]

Savings: 5 bytes × 2 entries = 10 bytes per page
```

### Bulk Load (Future)

For initial population, **bulk load** builds balanced tree bottom-up:

```
1. Sort all keys
2. Build leaf pages from sorted data
3. Build internal nodes level by level
4. No splits, optimal fill factor
```

**Benefit**: 2x faster for initial load, 95% page utilization

## Related Documentation

- [ADR-002: Copy-on-Write B+tree Strategy](/adr/002-copy-on-write-btree)
- [File Format Internals](/architecture/file-format)
- [Semantics V0: B+tree Invariants](/specs/semantics_v0)
- [DB API Reference](/reference/api)
